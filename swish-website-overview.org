#+TITLE:swish website 
#+AUTHOR: Ivan Hanigan
#+email: ivan.hanigan@anu.edu.au
#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [a4paper]
#+LATEX: \tableofcontents
-----

* structure

** home
*** COMMENT get img-code
#+name:get img
#+begin_src R :session *R* :tangle no :exports none :eval no
  ################################################################
  # name:get img
  if(file.exists("images/Fig1.png")) file.remove("images/Fig1.png")
  file.copy("~/Dropbox/projects/ANDS/proposal/version-0/Fig1.png", "images/Fig1.png")
  # edit scale with gimp
  file.copy("~/Dropbox/projects/ANDS/proposal/version-0/Fig1.png", "images/Fig1HiRes.png")
#+end_src
*** source
#+name:source-header
#+begin_src markdown :tangle index.md :exports none :eval no :padline no
---
layout: default
title: Extreme Weather Events Database
---

# This is the Extreme Weather Events Database of the:

    Scientific Workflow and Integration Software for Health (SWISH)
<p></p>
project from the [National Centre for Epidemiology and Population Health](http://nceph.anu.edu.au/) (NCEPH) at the Australian National University (ANU).  

The SWISH project consists of a suite of software tools that we have worked on and leverage off the work of the [https://kepler-project.org/](https://kepler-project.org/), the [http://postgis.refractions.net/](http://postgis.refractions.net/), [http://www.r-project.org/](http://www.r-project.org/) and the [http://www.stata.com/](http://www.stata.com/) software systems. Our work includes both an operational web-based research platform as well as enhanced traditional desktop client-side workflows, that boosts our researcher's capacity without compromising our expertise and trusted workflows.  You can read about the other elements of this project at the [Official Project Blog](http://swish-climate-impact-assessment.blogspot.com.au/) or this [High Level System Description Document webpage](/HighLevelDescription.html).  From those sites users can explore the scope of the project. This site is devoted to low level descriptions of the software resources included in our project.

## An example climate impact assessment workflow design
The image below is a diagram of the work that needs to be done (Clink here for [Hi Res](/images/Fig1HiRes.png)).  The implementation using our Scientific and Integration Software is in development at the moment.

![Fig1.png](/images/Fig1.png)

#+end_src

** documentation
*** documentation header
#+name:documentation-header
#+begin_src markdown :tangle documentation.md :exports none :eval no :padline no
  ---
  name: documentation
  layout: default
  title: documentation
  ---
  
  # SWISH Documentation
  
  SWISH is released under [the GPL license](http://www.opensource.org/licenses/gpl-license.php)
  
  Our software is distributed in the hope that it will be useful, but
  WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
  
  # Tutorials
  
  [Setting up your Environment](/setting-up.html)
  
  [Assembling Scientific Workflows](/assembling-workflows.html)
  
  [Developing with Workflow Software](/developing-with-workflows.html)
  
  # Admin
  
  [Administrative and technical details for the project](/admin.html)
#+end_src



* Posts
** drafts
*** testimonials
The Scientific Workflow and Integration Software for Health (SWISH)
provides an enviable collection of research tools for the conduct of
heath and social science research. The starting point for SWISH is the
integrated data catalogue, which provides an ideal access point for
finding and exploring spatial data available through SWISH.

Once data is discovered, the researcher then has the capacity to readily
access the relevant spatial data through the SWISH Extreme Weather
Events database (EWEDB). The integration of the Postgres/PostGIS
database and Geoserver web service for visualisation, along with the
streamlined access to the spatial data through the Rstudio server
environment, enable the integration of geospatial data with other survey
and administrative data sources.

This integration capability allows us to easily bring together data
sources that have not previously been considered in common, due to the
level of knowledge required, covering multiple disciplines and research
methods. In the example presented here, we provide a simple analysis of
the distribution of drought across NSW in 2006, derived from Bureau of
Meteorology data, and the vote for the Liberal Party in the same
electorates in 2010, drawn from the Australian Electoral Commission
election results website. The correlation between the level of drought
in 2006 and voting behaviour in 2010 is then shown in the concluding figure.

The integration of the system with GitHub, the DDIIndex data catalogue
and the SWISH data registry system also enable the research to be fully
documented, published and then available for reanalysis, further
demonstrating the potential of the system for supporting reproducible
research. All of the analysis presented here is available through the
project GitHub repository.

While the analysis is exploratory only, the use of the SWISH system
shows the ease with which multiple data sources can be brought together,
and hence to be able to answer more complex research questions, and at
increasingly specific levels of geography. Using this system,
researchers might then consider the effects of weather patterns on
social phenomena, such as the relationship between seasonal weather
patterns and depression within local government areas, or extreme
weather events and social media use.
I look forward to using this system further in future.

Steve McEachern, Deputy Director of the Australian Data Archive and
senior research fellow with the Australian Demographic and Social
Research Institute at the Australian National University.

*** 2013-06-08-creating-a-validated-events-database
**** Rcode

#+begin_src R :session *R* :tangle no :exports none :eval yes
  ################################################################
  # name:rcode
  fileslist <- dir("~/Pictures", pattern="eventdb", full.names=T)
  fileslist
  for(f in fileslist)
    {
  #    f  <- fileslist[1]
      file.rename(f,
                  file.path("images",basename(f))
                  )
    }
#+end_src

#+RESULTS:

**** markdown
#+begin_src markdown :tangle creating-a-validated-events-database.md :exports none :eval no :padline no
  ---
  name: 2013-06-08-creating-a-validated-events-database
  layout: default
  title: Creating a Validated Events Database
  categories:
  - validated events
  ---
  
  ## Creating a database for a validated historical record of events
  In a previous project Ivan was involved with the creation of a database of validated extreme bushfire pollution episodes.  This involved an integration of environmental data and content analysis of historical documentation.  This post will describe the database layout and in particular the approach taken to link ('tag'?) documentary evidence to event descriptions.  
  
  ## Why do we need to link multiple lines of evidence from documentary sources?
  The alternate approach to identify extreme events is to look at the distribution of the observed historical data and define an arbritrary threshold such as the 99th percentile an label any observations that achieve this threshold as 'extreme'.  This approach is limited by assumptions and data availabilty.  The use of multiple sources of information has the benefit of allowing analysts to explore the different dimensions of the events that make the considered 'extreme' by a broader spectrum of people.  For example a extreme heat event might achieve a criteria on one index say maximum daytime temperature, but not on others such as minimum night-time temperature or accumulated heat stress with no relief over several days.
  
  ## Step one: identify the key variables and summarise their historical distributions
  The ultimate aim of gathering references from multiple sources is to find evidence that supports the claim that any particular 'event' identified from the observed data is indeed considered extreme but the mutli-criteria reality of the phenomenon.  So in the case of the bushfire pollution case study we needed to find out what were the extreme pollution days.  Equivalently we would summarise the historical heat records for heatwaves or dryness records for drought.
  
  ## Step two: find supporting evidence that these event locations and dates were considered extreme
  In many cases government reports and news media are good sources for this.  Sometimes really extreme events might be assessed by scholars and entire reports or books might be written deconstructing the event and discussing the causal factors.
  
  ## How to keep track of all the references?
  Because we aim to support each individual event with multiple source references a relational database suggests itself.  In addition because we will want to capture our 'tagged' notes and explanatory discussions that expand on why we think each piece of evidence is relevant a data entry form is suggested so humans can easily add textual and numeric information.  As this is a large exercise that will require efforts by multiple people a distruted system with a central data store is recommended such that data entry is conducted using web-forms that connect the users to the central database.  
  
  ## Database solutions
  The exploration we conducted examined three databse solutions (presented in order of cost and open-ness of source code):
  
  - Web2Py
  - Oracle XE APEX
  - Microsoft Access
  
  We decided to go with Oracle XE Apex because of the lower level of expertise required to construct the web-forms.
  
  ## Setting up the Oracle XE server
  See Ivan's instructions at the OpenSoftware-RestrictedData website.  Create a workspace for your projects. 
  
  ## Setting up the Web-Forms
  These notes are from Ivan's explorations and serve more as a reminder of what worked rather than a description of why these steps should be conducted.  A thorough review of the Oracle XE apex tutorials will also help.
  
  
  ## Create Two Linked Tables using R/SQL
  The database relies on two tables: one for the events and the other for the references.  Creating these using the R/SQL method will allow the process to be scripted, giving better reproducibility and also extension later, follow these steps.
  
  - Connect R to the oracle db
  
  #### Code
      require(swishdbtools)
      require(ddiindexdb)
      pwd <- getPassword()
      ch <- connect2oracle(hostip = 'IP.ADDRESS', db = 'WORKSPACE', p = pwd)
  
  
  - Create sequences
  
  #### Code:
      dbSendUpdate(ch, 'CREATE SEQUENCE   "BUSHFIREEVENTS_SEQ"  
      MINVALUE 1 MAXVALUE 9999999999999999999999999999 
      INCREMENT BY 1 START WITH 1 CACHE 20 NOORDER  NOCYCLE')
  
  
      dbSendUpdate(ch, 'CREATE SEQUENCE   "BUSHFIREREFERENCES_SEQ"
      MINVALUE 1 MAXVALUE 9999999999999999999999999999
      INCREMENT BY 1 START WITH 1 CACHE 20 NOORDER  NOCYCLE')
    
  
  - Create tables with primary keys and foreign keys
  
  #### Code:
        dbSendUpdate(ch, 'CREATE TABLE  "BUSHFIREEVENTS" 
       (    "EVENTID" NUMBER NOT NULL ENABLE, 
            "EVENTTYPE" VARCHAR2(255), 
            "PLACE" VARCHAR2(255), 
            "MINDATE" DATE, 
            "MAXDATE" DATE, 
             CONSTRAINT "BUSHFIREEVENTS_PK" PRIMARY KEY ("EVENTID") ENABLE
       )')
  
      dbSendUpdate(ch, '
      CREATE TABLE  "BUSHFIREREFERENCES" 
      (    "REFID" NUMBER NOT NULL ENABLE, 
            "EVENTID" NUMBER NOT NULL ENABLE, 
            "AUTHOR" VARCHAR2(255), 
            "YEAR" VARCHAR2(255), 
            "TITLE" VARCHAR2(255), 
            "URL" VARCHAR2(255), 
            "NOTES" VARCHAR2(255), 
             CONSTRAINT "BUSHFIREREFERENCES_PK" PRIMARY KEY ("REFID") ENABLE
      )')
  
      dbSendUpdate(ch, '
      ALTER TABLE  "BUSHFIREREFERENCES" ADD CONSTRAINT "BUSHFIREREFERENCES_FK" FOREIGN KEY ("EVENTID")
                REFERENCES  "BUSHFIREEVENTS" ("EVENTID") ENABLE
      ')
  
  -   create triggers
  
  #### Code
      dbSendUpdate(ch, '
      CREATE OR REPLACE TRIGGER  "BI_BUSHFIREEVENTS" 
        before insert on "BUSHFIREEVENTS"               
        for each row  
      begin   
        if :NEW."EVENTID" is null then 
          select "EVENTS_SEQ".nextval into :NEW."EVENTID" from dual; 
        end if; 
      end;
      ')
  
      dbSendUpdate(ch, '
      ALTER TRIGGER  "BI_BUSHFIREEVENTS" ENABLE
      ')
  
    
      dbSendUpdate(ch, '
      CREATE OR REPLACE TRIGGER  "BI_BUSHFIREREFERENCES" 
        before insert on "BUSHFIREREFERENCES"               
        for each row  
      begin   
        if :NEW."REFID" is null then 
          select "REFERENCES_SEQ".nextval into :NEW."REFID" from dual; 
        end if; 
      end;
      ')
    
      dbSendUpdate(ch, '
      ALTER TRIGGER  "BI_BUSHFIREREFERENCES" ENABLE
      ')
  
  ### Now log in to the Oracle XE apex workspace
  
  ![eventdb-apex.png](/images/eventdb-apex.png)
  
  ### Navigate to the application builder
  
  - Click on "Create"
  - Select "Database" type
  - Click "from scratch" then Next
  - Give it a name, and ensure schema is selected to your workspace. Next
  - Add a blank page, then Next
  - One level of tabs, Next
  - No shared components, Next
  - Authentication scheme is Application Express, Next
  - Select a Theme, Next
  - Finally click "Create" to deploy the application
  
  ### Now start building the pages
  - Hit Create page
  - Choose form
  - Choose "form on a table with report", Next
  - Table Owner is our workspace, Table Name is "Events", (Choose from arrow type button)
  - Select the "Page Number" and set it so the report will go on the blank page (page 1), Next
  - Select all columns, Next
  - Select edit image, Next
  - Let Form page create a new page, rename to page2, Next
  - make sure the primary key is the eventid, Next
  - Define the source for pkey from existing sequence, select our EVENTS_SEQ from the list, Next
  - Select all columns, Next
  - Leave defaults on the Identify Process Options page, Next
  - Finish
  - Run page.
  - No data found.  Create, add a dummy value.  If everything works add another value.  EVENTID should increment by one.  All good?  Click on the Application ID below to return to edits.
  - create page, blank page, named as page 3, Next
  - name as references, Next
  - Say yes use tabs from existing set and create a new tab, Next
  - select the tabset, next
  - add tab label, Next, Finish.
  - Go to top level of application, Create Page
  - Form, Form on table with report
  - Table owner is our workspace
  - table is our references table, Next
  - set it to put the report on to page 3, Next
  - select all columns, Next
  - edit image, Next
  - create form on page 4 (this will be created), check pkey is refid, Next
  - select existing sequence, it is the REFS_SEQ, Next
  - select all columns, Next
  - Identify Process Options are all defaults, next
  - Finish and go to main application page
  
  ### Now we need to set up the link between events and related references
  - Create a hidden item on report page (page3) IE P3_EVENTID, Next
  - leave default, next
  - leave default, create item
  - Change report regions source SQL to 'WHERE EVENTID = :P3_EVENTID', apply changes
  - Set up the link on page1, go to report, report attributes, edit eventID, link, Use the buttons to set as shown in the image below then Apply changes
  
  ![eventdb-link.png](/images/eventdb-link.png)
  
  
  - In the References form page (page4) set the default value for say EVENTID to PL/SQL expression & = :P3_EVENTID, Apply changes
  - Run the application
  - Minor edits were needed to fix up the names in the tabset, and remove the breadcrumbs.
  
  ### set up some Data Input Rules
  - To set a list of Event Types
  - create a new record
  - edit the page, edit the item
  - change type to Select List, go to list of values
  - Create or edit static List of Values
  
  ## Now we have a pretty good first draft
  If you want to add a column for summary notes in the first table:
  
  #### Code:
      dbSendUpdate(ch,'
        alter table BUSHFIREEVENTS
        ADD NOTES varchar2(255)
        ')
  
  #### adding large amounts of text
  Varchar2(255) adds a standared text variable but adding large amounts of notes to the references table would be better with CLOB data type.
  
  #### Code:
      dbSendUpdate(ch,'
        alter table BUSHFIREREFERENCES
        add MISCNOTES CLOB
        ')
  
  ### add these new fields to the relevant pages
  - edit the page of eg page 2 
  - add item, text field, name = P2_NOTES, width = 150
  - source type = DB Column, create, run
  - add some notes
  - go to page 1, edit
  - report on table, add notes to SQL
  - NB You have requested to change the Interactive Report query. If you added columns to the query, they will not be displayed when the report is run. You will need to use the actions menu and either select the columns or click Reset.
  - go to actions menu, drop down menu, select columns, move notes across to second column
  
  
  
  ### TO export
  - app home, export
  - Database Application or Database Application Components
  - export application
  - save the application sql file to a folder.
  
  ### TODO
  - make the type of event a static list with dropdown menu
  
  ## References
  Johnston, F., Hanigan, I., Henderson, S., Morgan, G., Portner, T., Williamson, G., and Bowman, D. (2011). Creating an integrated historical record of extreme particulate air pollution events in Australian cities from 1994 to 2007. Journal of the Air & Waste Management Association, 61(4), 390. doi:10.3155/1047-3289.61.4.390
  
#+end_src
** posted
*** 2013-04-15-postgis-utils-on-windows header

#+begin_src markdown :tangle _posts/2013-04-15-postgis-utils-on-windows.md :exports none :eval no :padline no
---
name: 2013-04-15-postgis-utils-on-windows
layout: post
title: PostGIS utils on windows
date: 2013-04-15
categories: 
- PostGIS
---

The SWISH EWEDB server is a postgres database with the PostGIS add-on. 
Some of our tools require that the local client computer has some postgres software, but we don't need you to actually install anything.
An easy way to get these tools to work (especially for windows users) is to:

- 1 download the zips from the links below:

[http://www.enterprisedb.com/products-services-training/pgbindownload](http://www.enterprisedb.com/products-services-training/pgbindownload)

[http://download.osgeo.org/postgis/windows/pg92/postgis-pg92-binaries-2.0.2w64.zip](http://download.osgeo.org/postgis/windows/pg92/postgis-pg92-binaries-2.0.2w64.zip)

- 2 and unzip them, 
putting the files into:

    C:\pgutils
    

<p></p>
A tutorial with screenshots to make use of the GIS features of the EWEDB will follow in the future.

#+end_src
*** A SWISH user test report header
#+name:A SWISH user test report-header
#+begin_src markdown :tangle _posts/2013-04-19-a-swish-user-test-report.md :exports none :eval no :padline no
  ---
  name: A-SWISH-user-test-report
  layout: post 
  title: A SWISH user test report
  date: 2013-04-19
  categories:
  - Demonstration of value
  ---
  
  ## A SWISH testimonial
  Here is what a test user had to say about the EWEDB.
  
  Steve McEachern is Deputy Director of the Australian Data Archive and
  senior research fellow with the Australian Demographic and Social
  Research Institute at the Australian National University.
  
  The Scientific Workflow and Integration Software for Health (SWISH)
  provides an enviable collection of research tools for the conduct of
  heath and social science research. The starting point for SWISH is the
  integrated data catalogue, which provides an ideal access point for
  finding and exploring spatial data available through SWISH.
  
  Once data are discovered, the researcher then has the capacity to readily
  access the relevant spatial data through the SWISH Extreme Weather
  Events database (EWEDB). The integration of the Postgres/PostGIS
  database and Geoserver web service for visualisation, along with the
  streamlined access to the spatial data through the Rstudio server
  environment, enable the integration of geospatial data with other survey
  and administrative data sources.
  
  This integration capability allows us to easily bring together data
  sources that have not previously been considered in common, due to the
  level of knowledge required, covering multiple disciplines and research
  methods. In the example presented here, we provide a simple analysis of
  the distribution of drought across NSW in 2006, derived from Bureau of
  Meteorology data, and the vote for the Liberal Party in the same
  electorates in 2010, drawn from the Australian Electoral Commission
  election results website. The correlation between the level of drought
  in 2006 and voting behaviour in 2010 is then shown in the concluding figure.
  
  The integration of the system with GitHub, the DDIIndex data catalogue
  and the SWISH data registry system also enable the research to be fully
  documented, published and then available for reanalysis, further
  demonstrating the potential of the system for supporting reproducible
  research. All of the analysis presented here is available through the
  project GitHub repository.
  
  While the analysis is exploratory only, the use of the SWISH system
  shows the ease with which multiple data sources can be brought together,
  and hence to be able to answer more complex research questions, and at
  increasingly specific levels of geography. Using this system,
  researchers might then consider the effects of weather patterns on
  social phenomena, such as the relationship between seasonal weather
  patterns and depression within local government areas, or extreme
  weather events and social media use.
  I look forward to using this system further in future.
  
      Dr. Steven McEachern
      Deputy Director
      Australian Data Archive
      Australian National University
      Ph. +61 2 6125 2200
      http://www.ada.edu.au
      28 September 2012
#+end_src

*** 2013-04-26-extract-weather-from-grids
**** COMMENT get-pics-code
#+name:get-pics
#+begin_src R :session *R* :tangle no :exports none :eval yes
  ################################################################
  # name:get-pics
  picdir  <- "~/Pictures"
  flist  <- dir(picdir, "extract-data", full.names = F)
  for(f_i in flist)
    {
    #  f_i  <- flist[1]
      file.copy(file.path(picdir,f_i), file.path("images", f_i) )
    }
#+end_src

#+RESULTS: get-pics
**** post
#+name:2013-04-26-extract-weather-from-grids-header
#+begin_src markdown :tangle _posts/2013-04-26-extract-weather-from-grids.md :exports none :eval no :padline no
  ---
  name: 2013-04-26-extract-weather-from-grids
  layout: post
  title: Extracting Weather Data from Grids
  categories:
  - awap
  - extract
  ---
  
  # Gridded weather Data
  One of the cornerstone datasets in the EWEDB is the gridded weather data from the [Australian Bureau of Meteorology](http://www.bom.gov.au).  This post will describe a user extracting weather data for their study locations from overlaying the coordinates on a grid and returning the value of the pixels at that location for a specified date.
  
  ## Step one: find the data
  ### First log in to the Web Catalogue

  ![extract-data-login-ddiindex.png](/images/extract-data-login-ddiindex.png)

  ### Then Browse 

  ![extract-data-browse.png](/images/extract-data-browse.png)

  ### Or Search

  ![extract-data-search.png](/images/extract-data-search.png)
  
  ### These data are discovered.  Further information is available.

  ![extract-data-search-result.png](/images/extract-data-search-result.png)
  
  ## Step two: Create a Kepler Workflow
  
  The Workflow in the image below:
  
  - gets a list of study locations in the towns.xlsx file (Notice that Wolongong is MISSPELT?)
  - subsets them to the places of interest
  - geocodes them using the google geocoder (which will return a fuzzy logic best match for the misspelt name - thanks Google!)
  - uploads the coordinate data (in latitude and longitude) to the EWEDB PostGIS server (after checking our saved password in the postgres.conf file)
  - tells the PostGIS data are a points vector datatype, and that the coordinates are in GDA 1994 projection system
  - extracts the pixel values for the raster named in the string constant (that we found from the catalogue)
  
  ![setup-swish-Slide8.PNG](/images/setup-swish-Slide8.PNG)

  ![extract-data-kepler.png](/images/extract-data-kepler.png)
  
  ## The result
  The result is a file extracted from the database to the local TEMP directory and the name is shown.
  
  ![setup-swish-Slide13.PNG](/images/setup-swish-Slide13.PNG)
  
  The user can then take these data for further work
  
  ![setup-swish-Slide14.PNG](/images/setup-swish-Slide14.PNG)  
  
  ## Quality Control
  An imporant point to note is that the coordinates retrieved from the GoogleMaps geocoder might not be correct.  It is easy to check that the locations we just stored in the database are correct by viewing them in Quantum GIS (see [this previous post](/2013/04/quantumgis-and-postgis) for instructions on setting up Quantum GIS).
  
  ![setup-swish-Slide15.PNG](/images/setup-swish-Slide15.PNG)

  ![setup-swish-Slide16.PNG](/images/setup-swish-Slide16.PNG)
  
  Thankfully these locations appear good (even the mis-spelt "Wolongong").
#+end_src
  <!--![extract-data-kepler-ran.png](/images/extract-data-kepler-ran.png)-->
  <!--![extract-data-result.png](/images/extract-data-result.png)-->

*** 2013-04-27-set-up-swish-computer header
**** COMMENT get-pics-code
#+name:get-pics
#+begin_src R :session *R* :tangle no :exports none :eval yes
  ################################################################
  # name:get-pics
  picdir  <- "~/Pictures/Presentation1"
  flist  <- dir(picdir, full.names = F)
  flist
  for(f_i in flist)
    {
      #f_i  <-  flist[1]
      f_i2 <- paste("setup-swish-", f_i, sep ="")
      #f_i2
      file.copy(file.path(picdir,f_i), file.path("images", f_i2) )
    }
  
  for(f_i in flist)
    {
      #f_i  <-  flist[1]
      f_i2 <- paste("setup-swish-", f_i, sep ="")
      print(paste("![",f_i2,"](",f_i2,")", sep = ""))  
    }
#+end_src

**** post
#+name:set-up-swish-computer-header
#+begin_src markdown :tangle _posts/2013-04-27-set-up-swish-computer.md :exports none :eval no :padline no
  ---
  name: set-up-swish-computer
  layout: post
  title: Set Up your Swish Computer to connect to EWEDB
  date: 2013-04-27
  categories:
  - set up
  - connecting
  ---
  
  ## Install SWISH Database Tools R package
  In this tutorial the swishdbtools package will be set up so that you can connect to the EWEDB using the R tools we developed to allow your username and password to be used by [the SWISH Kepler actors](https://github.com/swish-climate-impact-assessment/swish-kepler-actors).  
  
  ### An important note is that whilst not a requirement there are a lot of the Kepler Project's features which use the R language and so this requires that R be on the users PATH.  
  
  The process shown below is on Windows XP with R 2.15.0 and has been tested with Ubuntu 12.04 with R 2.15.2 (but NOT TESTED ON MAC).
  
  We will install the necessary R package with the Database Drivers in it, and then the package is designed to also assist you to store a private copy of your username and password inside your profile on your computer.  This will be in a file called [.pgpass on linux and pgpass.conf on windows](http://www.postgresql.org/docs/current/static/libpq-pgpass.html).  On Microsoft Windows the file is named %APPDATA%\postgresql\pgpass.conf (where %APPDATA% refers to the Application Data subdirectory in the user's profile).  
    
  OPTIONALLY on Linux (for full functionality with psql)  the permissions on .pgpass must disallow any access to world or group; achieve this by the command:
  
      sudo chmod 0600 ~/.pgpass
  <p></p>
  ## Install the package binary file
  There are two main options.  Either install with devtools (if on windows you'll need Rtools installed):
  
      require(devtools)
      install_github("swishdbtools", "swish-climate-impact-assessment")
      require(swishdbtools) 
  <p></p> 
  OR download the package binaries from [the SWISH downloads page](http://swish-climate-impact-assessment.github.io/tools/swishdbtools/swishdbtools-downloads.html) and install using R.  The Rstudio software is shown here:
  
  ![setup-swish-Slide1.PNG](/images/setup-swish-Slide1.PNG)
  
  ## Browse to the downloads
  
  ![setup-swish-Slide2.PNG](/images/setup-swish-Slide2.PNG)
  
  ## Install SWISH Database Tools R package Dependencies:
  The swishdbtools package is still in development and is not on CRAN so you will need to sort out the dependencies yourself.  
  
  You can install all the dependencies with something like:
  
      install.packages(c("foreign", "rgdal", "plyr", "RODBC", "XLConnect"))
      require(swishdbtools)
    <p></p>
  OR if you don't want the newest versions
  
      if (length(grep("ming", sessionInfo()[[1]]$os)) == 1) {
        download.file("http://swish-climate-impact-assessment.github.io/tools/swishdbtools/swishdbtools_1.2.zip", 
                      destfile=file.path(Sys.getenv("HOME"), "swishdbtools_1.2.zip"), 
                      mode="wb")
        install.packages(file.path(Sys.getenv("HOME"), "swishdbtools_1.2.zip"), repos = NULL)
      } else {
        download.file("http://swish-climate-impact-assessment.github.io/tools/swishdbtools/swishdbtools_1.2_R_x86_64-pc-linux-gnu.tar.gz", 
                      destfile=file.path(Sys.getenv("HOME"), "swishdbtools_1.2_R_x86_64-pc-linux-gnu.tar.gz"), 
                      mode="wb")
        install.packages(file.path(Sys.getenv("HOME"), "swishdbtools_1.2_R_x86_64-pc-linux-gnu.tar.gz"), repos = NULL)
      }
      if(!require(foreign))   install.packages("foreign", repos="http://cran.csiro.au/"); require(foreign)
      if(!require(rgdal))     install.packages("rgdal", repos="http://cran.csiro.au/");     require(rgdal)  
      if(!require(plyr))        install.packages("plyr", repos="http://cran.csiro.au/");      require(plyr)
      if(!require(RODBC))       install.packages("RODBC", repos="http://cran.csiro.au/");     require(RODBC)
      if(!require(XLConnect)) install.packages("XLConnect", repos="http://cran.csiro.au/"); require(XLConnect)
      require(swishdbtools)
      ch <- connect2postgres2("ewedb")
      sql_subset(ch, "dbsize", limit = 1, eval = T)
  <p></p>
  
  ## Installing dependencies
  
  ![setup-swish-Slide3.PNG](/images/setup-swish-Slide3.PNG)
  
  ## should not have any ERRORS (but may have a few WARNINGS)
  
  ![setup-swish-Slide4.PNG](/images/setup-swish-Slide4.PNG)
    
  ![setup-swish-Slide7.PNG](/images/setup-swish-Slide7.PNG)
  
  ## While we are here, let's install Ivan's GisViz package so we run the [SWISH geocoder workflow](http://swish-climate-impact-assessment.github.io/tools/geocoder/geocoder.html) as a TEST
  
  The SWISH project has developed a [simple geocoder workflow](http://swish-climate-impact-assessment.github.io/tools/geocoder/geocoder.html) to assist our users to get going on a climate/health analysis. This depends on Ivan Hanigan's [GisViz package](http://ivanhanigan.github.io/gisviz/) so let's download and install that now.
   
  ## Download the binary and use Rstudio to browse to the downloaded file
  
  ![setup-swish-Slide5.PNG](/images/setup-swish-Slide5.PNG)
  
  ## Again we need to install the dependencies (when the packages are on CRAN this will be automatic)
  
      install.packages(
                        c("RCurl", "XML", "ggmap", "maps", 
                        "maptools", "RColorBrewer", "spdep", "rgdal")
                      )
      require(gisviz)
  <p></p>
  ![setup-swish-Slide6.PNG](/images/setup-swish-Slide6.PNG)
  
  ## Get the SWISH geocoder workflow from [this webpage](http://swish-climate-impact-assessment.github.io/tools/geocoder/geocoder.html)
  This is from version 2.4.  If you open this with Kepler 2.3 and don't want to upgrade, tell it to Force Open, then skip element and once open edit the SDF director and change AUTO to 1.
  
  To demonstrate the geocoder at work let's create some dummy data.  In a spreadsheet I've typed the names of a few towns in New South Wales. This uses the [GoogleMaps geocoding API (with HTTPS security)](https://developers.google.com/maps/documentation/geocoding) which is very clever at resolving place names using fuzzy logic.  To demonstrate this I have misspelt the name of the well-known city Wollongong, leaving off one of the L's and expect google maps to return the correct coordinates anyway. 
  
  ![setup-swish-Slide8.PNG](/images/setup-swish-Slide8.PNG)
  
  ## Modify the input file name to reflect the location of your spreadsheet
  
  ![setup-swish-Slide18.PNG](/images/setup-swish-Slide18.PNG)
  
  ## when you run this it will look for your PostGIS username and password, or ask you to enter them (ON WINDOWS THE POPUP BOX IS OFTEN BEHIND OTHER WINDOWS)
  
  You will have recieved a username and password when the Data Manager set up your account.
  
  ![setup-swish-Slide10.PNG](/images/setup-swish-Slide10.PNG)
  
  ![setup-swish-Slide11.PNG](/images/setup-swish-Slide11.PNG)
  
  ## your details are now stored in this file.
  (or ~/.pgpass on Linux) 
  
  ![setup-swish-Slide12.PNG](/images/setup-swish-Slide12.PNG)
    
  ## One thing this workflow does after geocoding the locations and storing a local shapefile is make a default map
  
  ![setup-swish-Slide19.PNG](/images/setup-swish-Slide19.PNG)
  
  ## But of more interest to us is the data it sent to the PostGIS database that we can view with Quantum GIS
    
  ![setup-swish-Slide15.PNG](/images/setup-swish-Slide15.PNG)
   
  ![setup-swish-Slide16.PNG](/images/setup-swish-Slide16.PNG)
    
  
  
  ## The End
  From here the interested reader can follow up on what that workflow does in [this previous post](/2013/04/extract-weather-from-grids/)
  
      
#+end_src

*** 2013-05-03 ExtractAWAPdata4locations header
#+name:ExtractAWAPdata4locations-header
#+begin_src markdown :tangle _posts/2013-05-03-extract-awap-data-4-locations.md :exports none :eval no :padline no
---
name: 2013-05-03-extract-awap-data-4-locations
layout: post
title: Extract AWAP data for locations
date: 2013-05-03
categories:
- awap
- extract
---

# AWAP data 
The AWAP data were found and extracted for a specific date in a previous post.
This tutorial will demonstrate extracting data for a range of dates and locations.

[See this page](/tools/ExtractAWAPdata4locations/extract-awap.html)

## Kaleen, ACT is a test case
In the attached example the study location is Kaleen, a suburb of Canberra.

![extract-kaleen.png](/images/extract-kaleen.png)
    
#+end_src

*** 2013-05-09-set-up-r header
**** COMMENT get-pics-code
#+name:get-pics
#+begin_src R :session *R* :tangle no :exports none :eval yes
  ################################################################
  # name:get-pics
  picdir  <- "~/Pictures/setupR"
  flist  <- dir(picdir, full.names = F)
  flist <- flist[-1]
  flist
  for(f_i in flist)
    {
      #f_i  <-  flist[1]
      f_i2 <- paste("setup-r-", f_i, sep ="")
      #f_i2
      file.copy(file.path(picdir,f_i), file.path("images", f_i2) )
    }
  
  for(f_i in flist)
    {
      #f_i  <-  flist[1]
      f_i2 <- paste("setup-r-", f_i, sep ="")
      print(paste("![",f_i2,"](/images/",f_i2,")", sep = ""))  
    }
#+end_src

**** post
#+name:set-up-swish-computer-header
#+begin_src markdown :tangle _posts/2013-05-09-set-up-r-on-ms-windows.md :exports none :eval no :padline no
  ---
  name: set-up-r-for-kepler-on-ms-windows
  layout: post
  title: Set Up R for Kepler on MS Windows
  date: 2013-05-09
  categories:
  - set up your environment
  ---
    
  ## Install R 3.0 
  Even if you have [The R Environment for Statistical Computing and Graphics](http://www.r-project.org/) installed we recommend you upgrade to version 3.0 because new packages from there will not work with R 2.15 etc.
  
  ## Register R in the PATH so that Kepler can find it
  This tutorial assumes windows 7 and a user without administrator privileges.
  
  ![setup-r-Slide1.PNG](/images/setup-r-Slide1.PNG)
  
  ## First download and install R to a location you can write to
  
  ![setup-r-Slide2.PNG](/images/setup-r-Slide2.PNG)
  
  ## It won't be recognised on your PATH
  Because you are not admin it will not be in your path.  Check this by opening the terminal (Run > cmd) and then type R.
  
  ![setup-r-Slide3.PNG](/images/setup-r-Slide3.PNG)
  
  ## Go to the control panel and navigate to the set environment variables
  
  ![setup-r-Slide4.PNG](/images/setup-r-Slide4.PNG)
  
  ## make a new USER variable 
  
  ![setup-r-Slide5.PNG](/images/setup-r-Slide5.PNG)
  
  ## Locate the appropriate R binaries
  
  ![setup-r-Slide6.PNG](/images/setup-r-Slide6.PNG)
  
  ## make the new variable called Path
  
  ![setup-r-Slide7.PNG](/images/setup-r-Slide7.PNG)
  
  ## Exit and restart the terminal and check that R is recognised
  
  ![setup-r-Slide8.PNG](/images/setup-r-Slide8.PNG)
  
  ## The End
  
      
#+end_src

*** 2013-05-13-awapgrids-vs-stations.md  header
#+name:2013-05-13-awapgrids-vs-stations.md -header
#+begin_src markdown :tangle _posts/2013-05-13-awapgrids-vs-stations.md :exports none :eval no :padline no
---
name: 2013-05-13-awapgrids-vs-stations.md 
layout: post
title: AWAP grids vs station observations 
---

## Comparing the gridded estimates to the observations

The EWEDB holds [daily gridded data we downloaded from BoM](/metadata/AWAP_GRIDS.html).  The size of this data collection is formidable (> 71,000 raster grids currently with 1980 to present, and set to grow significantly as we incoporate earlier decades). 

We were faced with the choice to store data for more time points (days), at lower spatial resolution (less megabytes) or for less time points at higher spatial resolution (more megabytes).  In the interest of deriving Extreme Weather Indices from the longest timeframe possible (to identify truly extreme observations from the full historical range) we decided to aggregate the original data from 5km pixels to 15kms squared pixels.  This loss of spatial precision is compensated to some extent by the high spatial autocorrelation as displayed in the map of the recent heatwave in New South Wales, Australia, January 2013.

![grid-nsw.png](/images/grid-nsw.png)

When we aggregate pixels of the grid there is more chance that the observed data will be different from the estimate at that location due to spatial smoothing.  In this post we will compare the observations from BoM weather stations with the daily values for the grid cell they intersect.

There are 939 weather stations that have valid observations for all three temperature, vapour pressure (humidity) and rainfall in the 1990-2010 period we also hold the data for.  To save a bit of time we'll only do a 10 percent random sample (93) of these shown below.

![selected-stations.png](/images/selected-stations.png) 
    
Getting the values for each station from the grid pixel it lies on we can construct an artifical timeseries as shown.

![sampled-timeseries-from-grid.png](/images/sampled-timeseries-from-grid.png) 

Merging these estimates with the observed data we can compare them and derive some summary statistics such as the R-squared.

## Max Temp

![maxave.png](/images/maxave.png)

## Min Temp

![minave.png](/images/minave.png)

## Rainfall

![totals.png](/images/totals.png)

## Vapour Pressure (humidity) 9am

![vprph09.png](/images/vprph09.png)

## Vapour Pressure (humidity) 3pm

![vprph15.png](/images/vprph15.png)

## Conclusions
The comparison presented here shows that the observations and AWAP gridded datasets that we have processed for storage in the EWEDB differ, due to the spatial smoothing that has occured in the processing undertaken for the EWEDB project. 

Users are asked to bear this in mind when considering the appropriateness of these data for their specific application.

#+end_src

*** 2013-05-15-geocoder header
#+name:geocoder-header
#+begin_src markdown :tangle _posts/2013-05-15-geocoder.md :exports none :eval no :padline no
---
name: geocoder
layout: post
title: geocoder
---

## A exemplar workflow for geocoding locations
The geocoder workflow at [this clink](/tools/geocoder/geocoder.html) is an example that takes a list of locations and returns a shapefile with the latitude and longitudes, as well as a map.

As you can see when you open the KAR file, this workflow expects an XLSX file to be linked in the first actor.

![geocoder-kar.png](/images/geocoder-kar.png)

## list your locations

![geocoder-xls](/images/geocoder-xls.png)

## run the workflow to create an image

![geocoder-xls](/images/geocoder-img.png)

## and a shapefile, stored in your temporary directory

![geocoder-xls](/images/geocoder-shp.png)

    
#+end_src


*** 2013-06-05-developing-with-workflows header
#+name:developing-with-workflows-header
#+begin_src markdown :tangle _posts/2013-06-05-developing-with-workflows.md :exports none :eval no :padline no
---
name: 2013-06-05-developing-with-workflows
layout: post
title: Developing new tools with Kepler workflows
---

## Developing new tools with Kepler workflows
If you can't get an actor from the core Kepler actors or our SWISH actor contributions then you have two choices:

- assemble current actors into a new 'composite' actor or
- develop a R/python/matlab/stata function to be a new actor.

If you go for option 2 therefore you have to develop your function to work with other Kepler actors.    There are a few tricks to doing this.  This post will show several different approaches available to develop a new Kepler R actor.

## 1 identify R function
There is probably an R function that does what you want.  If not start writing one.  If it is a really simple case of just using a current R function with simple input/output requirements you can write it straight into the Rexpression actor and add some ports... however anything more than a couple of lines can get buggy quickly, and this is not a good place to be debugging code.

## 2 write function in a script
Then , test/debug in an IDE like emacs, Rstudio or eclipse; and then deploy to Rexpression actor in workflow

## 3 source() your script from kepler
Similar to 2 but rather than copy the code to the actor just add 

    source('path/to/script.R')

<p></p>

to the actor.

## 4 write a package
Similar to 2 and 3 but the function is written to a package and then this is loaded with

    require(MyPackage)
    outputPortValue <- myFunction(inputPortValue, otherArgument)
    outputPortValue

You can then publish this on GitHub or CRAN, or even just send as a zip or tar to your collaborators.

## 5 add this to MyWorkflows
if you have the R code or package details in the actor save this by right clicking on the actor and save it to your kepler directory under MyWorkflows.  This means it will be available whenever you open Kepler.

## 6 If it is really awesome contribute it to SWISH
Take a fork of the swish-kepler-actors GitHub repo and add your actor and tests to the simpleInstaller/Actors folder. Then send a pull request to the SWISH maintainers and these will then be incorporated into our one-click installer.
    
#+end_src

** 2013-10-26-finishing-up
#+name:finishing-up-header
#+begin_src markdown :tangle ~/projects/swish-climate-impact-assessment.github.com/_posts/2013-10-26-finishing-up.md :exports none :eval no :padline no
---
name: finishing-up
layout: post
title: Finishing Up
date: 2013-10-26
---

- The SWISH project is finishing up.
- Ivan will add any future swishdbtoools or EWEDB content to his [Open Notebook Science (ONS) blog](http://ivanhanigan.github.io/)
- [Keith got a new job](https://globalhealth.duke.edu/people/faculty/dear-keith)
- ANDS showed our video at eResearch 2013.

![Keith_at_eResearch2013.jpeg](/images/Keith_at_eResearch_2013.jpeg)

Cheers!    
#+end_src

* metadata
** metadata home
** EWE
*** EWE header
#+name:EWE-header
#+begin_src markdown :tangle EWE.md :exports none :eval no :padline no
  ---
  name: EWE
  layout: default
  title: EWE
  ---
  
  ## The Datasets
  ### General Weather Data
  #### [Australian Water Availability Project](/metadata/AWAP_GRIDS.html)
  #### [Weather Exposures by Statistical Local Area (SLA)](/metadata/WEATHER_SLA.html)
  
  ### Bushfires
  #### [The U-TAS Biomass Smoke Project](http://ivanhanigan.github.com/bio_validated_bushfire_events)
  
  ### Droughts
  #### [The Hutchinson Index](https://github.com/ivanhanigan/HutchinsonDroughtIndex)
  #### [The NSW Dept Primary Industry Drought Declarations](/EWE/HutchinsonDroughtIndex/HutchinsonDroughtIndex.html)
  
  ### Heatwaves
  #### [The Excess Heat Factor](https://github.com/swish-climate-impact-assessment/ExcessHeatIndices)
  
  
  
#+end_src

** AWAP_GRIDS
*** AWAP_GRIDS header
#+name:AWAP_GRIDS-header
#+begin_src markdown :tangle metadata/AWAP_GRIDS.md :exports none :eval no :padline no
---
name: AWAP-GRIDS
layout: default
title: AWAP GRIDS
---

The Australian Water Availability Project Gridded Weather Data.

Measure can be maxave, minave, (temperature), or vprph09, vprph15 (vapour pressure) 

For example: 

    awap_grids.maxave_20130108 

<p></p>

STUDY DESCRIPTION: 

The Bureau of Meteorology has generated a range of improved meteorological analyses for Australia as a contribution to the Australian Water Availability Project (AWAP). The meteorological analyses include rainfall, temperature, vapour pressure and solar exposure. Also Normalized Difference Vegetation Index (NDVI) and Atmospheric circulation. Over time periods ranging from daily, weekly, monthly to 3-yearly. 

[http://www.bom.gov.au/jsp/awap/](http://www.bom.gov.au/jsp/awap/) 

Documentation is at [http://www.bom.gov.au/amm/docs/2009/jones.pdf](http://www.bom.gov.au/amm/docs/2009/jones.pdf)

## Get the data
[Here is a workflow for extracting the data](/tools/ExtractAWAPdata4locations/extract-awap.html)
    
#+end_src
